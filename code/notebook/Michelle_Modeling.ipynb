{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sustained-ocean",
   "metadata": {},
   "source": [
    "# Modeling (Take 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "instructional-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model      import LinearRegression\n",
    "from sklearn.metrics           import *\n",
    "from sklearn.model_selection   import train_test_split\n",
    "from sklearn.metrics           import r2_score\n",
    "from sklearn.metrics           import accuracy_score\n",
    "from sklearn.linear_model      import Ridge\n",
    "\n",
    "import pickle\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parents[0]))\n",
    "import pandas as pd\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, String, Integer\n",
    "from config import Config\n",
    "\n",
    "from fuzzy_match import match\n",
    "from fuzzy_match import algorithims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-clark",
   "metadata": {},
   "source": [
    "### Accessing Amazon db \n",
    "for sample data to test on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "honest-internship",
   "metadata": {},
   "outputs": [],
   "source": [
    "### From config.py\n",
    "import os\n",
    "# basedir = os.path.abspath(os.path.dirname(__file__))\n",
    "\n",
    "class Config(object):\n",
    "    SECRET_KEY = os.urandom(24) # For WTF forms\n",
    "    SQLALCHEMY_DATABASE_URI = 'postgresql://consha_admin:consha8dev@msds603.cm9lzsru7xeh.us-west-2.rds.amazonaws.com/conshadb'\n",
    "    SQLALCHEMY_TRACK_MODIFICATIONS = True # flash-login uses session which require a secret\n",
    "\n",
    "conn = create_engine(Config.SQLALCHEMY_DATABASE_URI)\n",
    "\n",
    "engine = db.create_engine(Config.SQLALCHEMY_DATABASE_URI)\n",
    "connection = engine.connect()\n",
    "metadata = db.MetaData()\n",
    "Table = db.Table('amazon_product_500', metadata, schema='cached_data', autoload=True, autoload_with=engine) # why doesn't this work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-tattoo",
   "metadata": {},
   "source": [
    "Sample Amazon Ingredient Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "seventh-particular",
   "metadata": {},
   "outputs": [],
   "source": [
    "amz_df = pd.read_sql_table('amazon_product_500', con=connection, schema=\"cached_data\", index_col=None)\n",
    "sample_amz_ingredient_list = amz_df.ingredient[0].split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-behalf",
   "metadata": {},
   "source": [
    "### Next Step: converting those ingredients to scores by \"looking up\" each ingredient in the ewg database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "honey-rebecca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  1.  0.  0.  0.  0.  0.  0.  0.  1.5 2. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2.55747064])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model      import LinearRegression\n",
    "from sklearn.metrics           import *\n",
    "from sklearn.model_selection   import train_test_split\n",
    "from sklearn.metrics           import r2_score\n",
    "from sklearn.metrics           import accuracy_score\n",
    "from sklearn.linear_model      import Ridge\n",
    "\n",
    "import pickle\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "sys.path.append(str(Path.cwd().parents[0]))\n",
    "import pandas as pd\n",
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, String, Integer\n",
    "from config import Config\n",
    "\n",
    "from fuzzy_match import match\n",
    "from fuzzy_match import algorithims\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import os\n",
    "# basedir = os.path.abspath(os.path.dirname(__file__))\n",
    "\n",
    "class Config(object):\n",
    "    SECRET_KEY = os.urandom(24) # For WTF forms\n",
    "    SQLALCHEMY_DATABASE_URI = 'postgresql://consha_admin:consha8dev@msds603.cm9lzsru7xeh.us-west-2.rds.amazonaws.com/conshadb'\n",
    "    SQLALCHEMY_TRACK_MODIFICATIONS = True # flash-login uses session which require a secret\n",
    "\n",
    "conn = create_engine(Config.SQLALCHEMY_DATABASE_URI)\n",
    "engine = db.create_engine(Config.SQLALCHEMY_DATABASE_URI)\n",
    "\n",
    "filename = 'finalized_LR_model.sav'\n",
    "lr = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "def formatted_ewg_ingredient_score():\n",
    "    \"\"\"calls the ewg ingredient database and creates array of\n",
    "    ingredient, score lists.The scores are calculated as the mean\n",
    "    score for that ingredient,because some ingredients have multiple scores.\"\"\"\n",
    "    metadata = db.MetaData(schema='crawled_data')\n",
    "    ewg = db.Table('ewg_product', metadata, autoload=True, autoload_with=engine)\n",
    "    query = db.select([ewg])\n",
    "    result = conn.execute(query).fetchall()\n",
    "\n",
    "    ewg_df = pd.DataFrame(result, columns = ewg.columns.keys())\n",
    "\n",
    "    # taking mean of score per ingredient as ingredient score\n",
    "    ingredient_scores = ewg_df.groupby('ingredient')['ingredient_score'].apply(np.mean)\n",
    "    ingredient_scores = np.array(pd.DataFrame(ingredient_scores).reset_index())\n",
    "    return ingredient_scores\n",
    "\n",
    "#### Fuzzy Matching to match ingredients from amazon with scores from ewg ####\n",
    "def string_matching(ewg_string, amazon_ingredient):\n",
    "    \"\"\"Matches ewg ingredient strings with amazon ingredient strings.\"\"\"\n",
    "    match_score = algorithims.cosine(ewg_string.strip().lower(), amazon_ingredient.strip().lower())\n",
    "    if match_score > .70:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def ingredient_string_matching(raw_amz_ingredient_list, ewg_ingredient_scores):\n",
    "    \"\"\"Input: raw list of ingredients from amazon - list[str] and\n",
    "       numpy array of ewg ingredients and score pairs\n",
    "       Output: list of scores from ewg database - float\"\"\"\n",
    "\n",
    "    ingredient_score_list = []\n",
    "    for ingredient in raw_amz_ingredient_list:\n",
    "        for ewg_ingredient, ewg_score in ewg_ingredient_scores:\n",
    "            if string_matching(ingredient, ewg_ingredient) == True:\n",
    "                ingredient_score_list.append((ewg_score))\n",
    "                break\n",
    "    return ingredient_score_list\n",
    "\n",
    "# second feature: max_three_mean\n",
    "def max_three_scores(ingredient_scores):\n",
    "    \"\"\"Creating feature: mean of top three ingredients\"\"\"\n",
    "    ingredient_scores.sort(reverse=True)\n",
    "    return np.mean(ingredient_scores[:3])\n",
    "\n",
    "# third feature(s): a count of each value - going to take int of each\n",
    "def product_score_count(ingredient_scores):\n",
    "    \"\"\"Creating score count features\"\"\"\n",
    "    integer_ingredient_scores = [int(i) for i in ingredient_scores]\n",
    "    count_dictionary = Counter(integer_ingredient_scores)\n",
    "\n",
    "    # filling in the gaps with 0s\n",
    "    for i in range(1,10):\n",
    "        if not count_dictionary.get(i):\n",
    "            count_dictionary[i] = 0\n",
    "    return count_dictionary\n",
    "\n",
    "def combine_features(sample_amz_ingredient_list):\n",
    "    ewg_ingredient_scores = formatted_ewg_ingredient_score()\n",
    "    ingredient_scores = ingredient_string_matching(sample_amz_ingredient_list, ewg_ingredient_scores)\n",
    "    # creating features\n",
    "    max_three = max_three_scores(ingredient_scores)\n",
    "    ingredient_count = len(sample_amz_ingredient_list)\n",
    "    count_dictionary = product_score_count(ingredient_scores)\n",
    "    # combining and formatting features for model\n",
    "    x_df = pd.DataFrame(count_dictionary, index=[0])\n",
    "    x_df['max_three'] = max_three\n",
    "    x_df['ingredient_count'] = ingredient_count\n",
    "    x_df.columns = ['ingredient_count', 'max_three_mean', 'count_1', 'count_2', 'count_3',\n",
    "   'count_4', 'count_5', 'count_6', 'count_7', 'count_8', 'count_9']\n",
    "    return x_df.values.reshape(1,-1)\n",
    "    \n",
    "def model(sample_amz_ingredient_list):\n",
    "    return lr.predict(combine_features(sample_amz_ingredient_list))\n",
    "\n",
    "sample_amz_ingredient_list = ['water','oil']\n",
    "model(sample_amz_ingredient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "injured-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_LR_model.sav'\n",
    "lr = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "thirty-simulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41694587,  0.54287262, -0.43370836, -0.30515408, -0.30457866,\n",
       "       -0.29975584, -0.39117635, -0.45509677, -0.38973824,  0.44201264,\n",
       "        0.13735211])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-niagara",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acoustic-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_y(row): \n",
    "    \"\"\"converts y to numerical target by converting 'verified' to 0.\"\"\"\n",
    "    if 'verified' in row: \n",
    "        return 0\n",
    "    else:\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "applied-fashion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data \n",
    "ewg = pd.read_csv(\"/Users/michellejanneycoyle/Desktop/Crawling/Data/EWG_product.csv\")\n",
    "moisturizer_data = pd.read_csv(\"/Users/michellejanneycoyle/Desktop/Crawling/moisturizer_data.csv\")\n",
    "main_df = pd.concat([ewg, moisturizer_data])\n",
    "\n",
    "main_df['ingredient_score'] = main_df['ingredient_score'].apply(float)\n",
    "main_df['product_score'] = main_df.apply(lambda row: clean_y(row['product_score']), axis=1)\n",
    "main_df['product_score'] = main_df['product_score'].apply(float)\n",
    "\n",
    "product_overview = pd.DataFrame(main_df.groupby('product_name')['ingredient_score'].apply(list))\n",
    "product_overview['product_score'] = main_df.groupby('product_name')['product_score'].apply(np.mean).apply(int)\n",
    "product_overview['ingredient_count'] = product_overview['ingredient_score'].apply(lambda x: len(x))\n",
    "product_overview['max_three'] = product_overview.ingredient_score.apply(lambda x: np.sort(x)[-3:])\n",
    "product_overview['max_three_mean'] = product_overview['max_three'].apply(np.mean)\n",
    "for i in range(1, 10):\n",
    "    product_overview[f'count_{i}'] = product_overview['ingredient_score'].apply(lambda x: x.count(i))\n",
    "y = product_overview['product_score']\n",
    "\n",
    "X = product_overview.drop(['ingredient_score', 'product_score', 'max_three'], axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "literary-german",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6394140185633118 0.7379192615184421\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "MAE = mean_absolute_error(lr.predict(X_test), y_test)\n",
    "R_squared = r2_score(lr.predict(X_test), y_test)\n",
    "print(MAE, R_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-component",
   "metadata": {},
   "source": [
    "### Pickling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "clear-olive",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_LR_model.sav'\n",
    "pickle.dump(lr, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fundamental-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_LR_model.sav'\n",
    "lr = pickle.load(open(filename, 'rb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
